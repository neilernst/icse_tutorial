---
title: "ICSE-tutorial"
author: "Torkar, Furia, and Feldt"
date: "3/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
library(rethinking)
library(truncnorm)
set.seed(100)
```

## Fake data simulation

To have a simple and transparent example we'll first generate data. This is something one always should do, before even seeing any empirical data.

Assume that we have a complexity measure that we want to predict given lines of code (LOC). The outcome $y$, which is our complexity measure, is a real value ($\mathbb{R}$) that theoretically goes from $0$ to $100$. On the other hand, lines of code goes from $1$ to a large number (perhaps many thousands).

What assumptions can we make about the data-generation process, i.e., how $y$ came to be? Well, there are ontological and epistemological reasons to assume that the data comes from a $\mathsf{Normal}$ (i.e., Gaussian) distribution. 

Concerning the ontological reason, one could assume that the measure is generated by adding many fluctuations (experience of the person who wrote the code, language used, code conventions and other processes applied, etc.) On the other hand, the epistemological reason is founded on information theory and the concept of maximum entropy. In short, if the most straightforward way to explain data is to use its mean and variance then the $\mathsf{Normal}$ distribution is the best one to use, i.e., it has maximum entropy, which implies that it allows things to happen in the most ways and constrains data the least.^[One could also claim that a $\mathsf{Log}$-$\mathsf{Normal}$ distribution could be suitable since it would only allow $\mathbb{R}^+$.]

First, generate $1000$ random numbers from a \mathsf{Normal} distribution with a lower and upper bound of $0$ and  $100$, respectively. We set the mean to $50$ and the standard deviation to $15$.

```{r}
# our outcome
y <- rtruncnorm(1000, a = 0, b = 100, mean = 50, sd = 15)
```

Let's do the same for lines of code. We pick uniformly a number between $1$ and $1000$, and we do that $1000$ times with replacement. Then we store $y$ and $x$ in a data frame.

```{r}
# our predictor lines of code
x <- sample(seq(1:1e3), 1000, replace = TRUE) 

d <- data.frame(
  y = y,
  x = x
)
```

Descriptive statistics of the data we generated randomly.

```{r}
precis(d)
```


### Prior predictive checks

What priors should we use for our parameters? Do we have *any* prior knowledge? Yes, we actually do. We know that the theoretical measure should not be negative or above $100$.

Let's try some common default priors from literature, i.e., $\mathsf{Normal}(0,10)$ for the intercept $\alpha$ and $\mathsf{Normal}(0,1)$ for our slope $\beta$.

```{r}
N <- 100
a <- rnorm(N, 0, 10)
b <- rnorm(N, 0, 1)
plot(NULL, xlim=range(d$x), ylim=c(-200,200), xlab="LOC", ylab="Measure")
abline(h=0, lty=2)
abline(h=100, lty=2)

mtext( "a ~ dnorm(0,10); b ~ dnorm(0,1)" )
xbar <- mean(d$x)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) , # center the predictor
    from=min(d$x) , to=max(d$x) , add=TRUE ,
    col=col.alpha("black",0.2) )
```

The dashed lines indicate our theoretical maximum and minimum values for our measure. This is clearly not ok, all lines are outside our theoretical max and min values. We can do better than this. By conducting prior predictive checks we can propose some alternative priors.

Here are some priors which we think are better suited. First, set the mean for our intercept, $\alpha$, to $50$, which is between $0$ and $100$, and let's use a standard deviation of $10$. Second, use $\mathsf{Log}$-$\mathsf{Normal}(-4,1)$ for our $\beta$ parameter.

```{r}
set.seed(100)
N <- 100
a <- rnorm(N, 50, 10)
b <- rlnorm(N, -5, 2)
plot(NULL, xlim=range(d$x), ylim=c(-200,200), xlab="LOC", ylab="Measure")
abline(h=0, lty=2)
abline(h=100, lty=2)

mtext( "a ~dnorm(50,10); b ~ dlnorm(-5,2)" )
xbar <- mean(d$x)
for ( i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
    from=min(d$x) , to=max(d$x) , add=TRUE ,
    col=col.alpha("black", 0.2) )
```

Still some lines showing improbable values, but significantly better than before. The bulk of the lines are within the theoretical maximum and minimum values.

### Sample with fake data

Design a first model, which we'll use to recover our parameters that we used for generating the random data.

```{r, warning=FALSE, message=FALSE}
set.seed(100)
d$x <- d$x - mean(d$x) # center predictor

m <- ulam(
  alist(
    y ~ dnorm(mu, sigma),
    mu <- alpha + b_loc * x,
    alpha ~ dnorm(50, 10),
    b_loc ~ dlnorm(-5, 2),
    sigma ~ dexp(1)
), data = d, cores = 4, chains = 4, cmdstan = TRUE, refresh = 0, iter = 5e3)
```

Check the parameter estimates.

```{r}
round(precis(m), 2)
```

`alpha` is estimated to `r round(precis(m)[1,1],2)` (in our randomly generated sample it's `r round(mean(d$y),2)`), while the `sigma` parameter is estimated to `r round(precis(m)[3,1],2)`, which is close to what we used when generating random numbers (i.e., $15$). `b_loc` is very close to zero, as one could expect. In summary, we have decent recovery.

## Using empirical data

First load our data.

```{r}
d <- read.csv("data.csv")
```

Show descriptive statistics.

```{r}
precis(d)
```

So, $n=$ `r nrow(d)`, and our outcome $\bar{y} =$  `r round(precis(d)[1,1],2)`, while $\bar{x} \approx 320$.

Sample two models using empirical data. One is the null model, $\mathcal{M}_0$, consisting only of an intercept. The other is a model with our predictor, $\mathcal{M}_1$.

```{r, warning=FALSE, message=FALSE}
set.seed(100)
d$x <- d$x - mean(d$x) # center the values

m0 <- ulam(
  alist(
    y ~ dnorm(mu, sigma), # y is our outcome, i.e., the measure
    mu <- alpha, # only intercept
    alpha ~ dnorm(50, 10),
    sigma ~ dexp(1)
), data = d, cores = 4, chains = 4, cmdstan = TRUE, refresh = 0, iter = 5e3, log_lik = TRUE)

m1 <- ulam(
  alist(
    y ~ dnorm(mu, sigma), # y is our outcome, i.e., the measure
    mu <- alpha + b_loc * x, # x is our predictor LOC
    alpha ~ dnorm(50, 10),
    b_loc ~ dlnorm(-5, 2),
    sigma ~ dexp(1)
), data = d, cores = 4, chains = 4, cmdstan = TRUE, refresh = 0, iter = 5e3, log_lik = TRUE)
```

```{r}
c <- compare(m0, m1, func=LOO)
round(c, 2)
```

Well, well$\ldots$ It seems that adding the predictor doesn't make much difference concerning out of sample predictons. The difference in the information criterion $\mathrm{dPSIS} is `r c[2,3]`, while the difference in standard error is `r c[2,4]`. We can calculate a 95% interval for this,

```{r}
# dPSIS + interval * dSE * 95% z-value
c[2,4] + c(-1,1) * c[2,4] * 1.96
```

The interval is clearly crossing zero and, thus, we conclude that adding the predictor LOC doesn't do much for out of sample predictions compared to only have a null model with a general intercept.

If one would be interested in measuring the effect of LOC, then it might still be worthwhile keeping it in a model. 

In software engineering researchers seems to be worried about **omitted variable bias**, i.e., we are worried about missing a variable which we'd like to include, leading to studies including lots of variables, which is questionable at best. The problem is that **included variable bias** is just as dangerous where, at many times, confounding variables might sneak in and affect the estimations.

As a final note, let's compare the estimates we got from both models,

```{r}
round(precis(m0), 2)
round(precis(m1), 2)
```

For both models `alpha` and `sigma` are estimated very similarly. For $\mathcal{M}_1$, the intercept `alpha` is estimated to be `r round(precis(m1)[1,1], 2)` with a $95$% Credible Interval between `r round(precis(m1)[1,3], 2)` and `r round(precis(m1)[1,4], 2)`, while our slope, `b_loc` is estimated to `r round(precis(m1)[2,1], 2)` (we would claim that even though `b_loc` $95$% CI does not cross zero, i.e., columns $3$--$4$, there is likely no effect). Finally, `sigma` is estimated to be `r round(precis(m1)[3,1], 2)` $\mathrm{CI}_{95\%}$ [`r round(precis(m1)[3,3], 2)`,`r round(precis(m1)[3,4],2)`].

Taking only out of sample predictions into account and forced to pick "one model to rule them all", we would opt for $\mathcal{M}_0$. Why? It provides virtually the same out of sample prediction capabilities, while being simpler and easier to understand.

